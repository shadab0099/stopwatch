name: Auto-generate sitemap & robots.txt

on:
permissions:
  contents: write
  push:
    branches:
      - main
  schedule:
    - cron: "0 0 * * 0"

jobs:
  build-sitemap:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate sitemap.xml automatically
        run: |
          echo '<?xml version="1.0" encoding="UTF-8"?>' > sitemap.xml
          echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">' >> sitemap.xml

          find . -type f -name "*.html" | while read file; do
            url="https://stopwatch.calandtimesolutions.com/${file#./}"
            url=$(echo "$url" | sed 's/index\.html$//')
            echo "  <url><loc>${url}</loc><lastmod>$(date +%F)</lastmod><priority>0.8</priority></url>" >> sitemap.xml
          done

          echo '</urlset>' >> sitemap.xml

      - name: Create robots.txt
        run: |
          echo 'User-agent: *' > robots.txt
          echo 'Allow: /' >> robots.txt
          echo '' >> robots.txt
          echo 'Sitemap: https://stopwatch.calandtimesolutions.com/sitemap.xml' >> robots.txt

      - name: Commit and push SEO files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add sitemap.xml robots.txt
          git diff --cached --quiet || git commit -m "Auto-update sitemap & robots.txt"
          git push
